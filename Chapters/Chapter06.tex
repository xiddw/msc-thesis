%!TEX root = ../Thesis.tex

\chapter{Experimentos y resultados}\label{ch:chap6}

\epigraphhead[70]{
\epigraph{``Data! Data! Data!'' he cried impatiently. 
``I can't make bricks without clay.''}{Sherlock Holmes}}

En los capítulos anteriores se ha descrito los diferentes algoritmos que se utilizarán para realizar la tarea de \sd, y que en esta sección se emplearán de acuerdo al marco experimental que se describe a continuación.

Inicialmente, las pruebas consistieron en usar los algoritmos presentados para selección de modelo usando datos que fueron generados aleatoriamente a partir de los parámetros de un \ac{HMM} inicial; para tener una idea general de su desempeño individual.

Para estas primeras pruebas, se simuló una cadena de Márkov oculta con base en parámetros fijos, generando tanto una secuencia de datos observados, como los supuestos datos o variables ocultas que forman la cadena de Márkov. Se utilizó muestreo ancestral para la simulación de estos datos.

Para un caso en específico, se tiene lo siguiente. Realizando la inferencia de parámetros del \ac{HMM}, se obtienen los siguientes resultados:

El primer algoritmo que se prueba, es el de selección de modelo usando un \ac{BIC}.

Como ya se comentó, se usará una variante de \ac{BIC} en donde se incorpora un término de regularización $\lambda$ para que correspondan en órdenes de magnitud tanto la log-verosimilitud del modelo encontrado como su penalización respectiva.

El problema inmediato que se presenta, es cómo realizar la selección del parámetro de regularización $\lambda$ que penalice de forma correcta la verosimilitud para los diferentes modelos propuestos. Si $\lambda$ es demasiado pequeño, entonces la penalización realmente no tendrá efecto y dado el sobre ajuste que se presenta al usar modelos más complejos, se preferirán siempre los modelos con más parámetros. Por otro lado, si al escoger $\lambda$ se da demasiado peso al término de regularización, entonces siempre se preferirán los modelos más sencillos.

Para encontrar el valor de $\lambda$ adecuado, se puede entonces formar una superficie con las diferentes curvas de selección \ac{BIC} de acuerdo a cómo varía $\lambda$, e inspeccionar esta superficie para encontrar una región de confianza en la que el valor de $\lambda$ es el adecuado.

Por otro lado, para la segunda prueba, se procedió a usar bootstrap con la estadística \ac{LLR} como ya se describió anteriormente en el \autoref{ch:chap3}, y haciendo la prueba de hipótesis del modelo de $n$ estados contra el de $n+1$ estados.

\section{Experimentos} % (fold)
\label{sec:experimentos}

Para los experimentos realizados, se generaron mediante un \ac{TTS} para la generación de las secuencias de prueba, lo que nos permitió tener un mayor control sobre el contenido como tal de las grabaciones, así como sobre los posibles ruidos o interferencias en la señal de audio.

Si bien, para probar el desempeño contra otras propuestas del estado del arte se suelen usar otro tipo de bases de datos (tales como \ac{NIST}, ...), éstas suelen no estar disponibles de forma libre, por lo que preferimos generar nosotros un conjunto de pruebas con el sintetizador de voz.

Usando dos motores para el sintetizador de voz, uno con voces en inglés y otro con voces en español, se generaron 6 secuencias de audio (3 en cada idioma) cuya duración así como el número de interlocutores que participan varía.

\subsection[Secuencia 1]{Secuencia 1: Edgar Allan Poe} % (fold)
\label{ssub:allanpoe}

  \input{Chapters/Chapter06_01} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection[Secuencia 2]{Secuencia 2: Gabriel García Márquez} %(fold)
\label{ssub:soledad}

  \input{Chapters/Chapter06_02} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection[Secuencia 3]{Secuencia 3: William Shakespeare}
\label{ssub:lear}

  \input{Chapters/Chapter06_03} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection[Secuencia 4]{Secuencia 4: Manuel Acuña}
\label{ssub:nocturno}

  \input{Chapters/Chapter06_04} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection[Secuencia 5]{Secuencia 5: Calderón de la Barca}
\label{ssub:calderon}

  \input{Chapters/Chapter06_05} 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection[Secuencia 6]{Secuencia 6: Andrew Lloyd Webber}
\label{ssub:cats}

  \input{Chapters/Chapter06_06}   


\section{Resultados}

A continuación se presentan la tabla \autoref{tab:example} los resultados condensados de las pruebas realizadas con las secuencias antes descritas.

\begin{table}
  \centerline {
  \begin{tabularx}{1.2\textwidth}{X rrrccc} \toprule
    \tableheadline{Sec. Id.}& 
  	\tableheadline{Duración} & 
  	\tableheadline{$\#_{SAMPLES}$} &
  	\tableheadline{$\#_{WORDS}$} &  	
  	\tableheadline{$n_{TRUE}$} &
  	\tableheadline{$n_{FOUND}$} & 
    \tableheadline{DER} 	
	  \\ \midrule
    ALLPOE~ & 12:06 min & 7218 & 140 & 6 & 6 & 0.03324 \\ 
    GARMÁR~ &  6:55 min & 4154 &  90 & 4 & 4 & 0.00652 \\
    WSHAKE~ &  2:43 min & 1636 & 160 & 4 & 4 & 0.03324 \\ 
    MACUÑA~ & 10:41 min & 6414 & 120 & 3 & 3 & 0.00343 \\ 
    CALDER~ &  1:07 min &  676 & 160 & 3 & 3 & 0.08092 \\ 
    LLOYDW~ &  6.48 min & 4084 & 160 & 5 & 5 & 0.01170 \\    
    \bottomrule
  \end{tabularx} 
  }
  \caption[as]{asdasd}  
  \label{tab:example}
\end{table}
